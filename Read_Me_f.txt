Part f)

What gives us more information about a single die, Z or S?

Basically, we need to compare, I(X;Z) and I(X;S)

From the result snaps d and e for part d and part e, we see that, for both M=3 and M=6 we have the following :

I(X;Z) > I(X;S)

Thus, knowing about the product of the of X and Y reduces entropy in X much more than knowing about its sum does. 

This means that the sum, Z = XY, tells us more about X (or Y, either one)

Note :

I(X;Z) = H(X) - H(X|Z)

And 

I(X;S) = H(X) - H(X|S)

implying that larger the value of mutual information, larger is the reduction in entropy given that we know the sum or the product.

Since I(X;Z) > I(X;S),
we say knowing about Z = XY tells us more about X and reduces uncertainty in X more than knowing about S = X + Y does.

Thus Z is more informative about X (or Y) as compared to S.